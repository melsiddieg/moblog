---
title: "Efficient Data wrangling in R"
author: "Mohammed Abdallah"
categories: [big data, polars, duckdb,apache arrow]
---

![](plot2.png){fig-alt="Big Data"}

# Advanced Techniques for Efficient Large Data Handling in R

In the era of big data, R users often find themselves grappling with datasets that push the limits of their hardware. Traditional approaches using packages like tidyverse or data.table can struggle when faced with truly massive datasets. But fear not! A new generation of tools and techniques is emerging, offering R users powerful ways to handle large data efficiently. In this post, we'll explore some of these advanced techniques, focusing on three key players in the R ecosystem: Arrow, Polars, and DuckDB.

## The Power of Efficient File Formats and Partitioning

Before diving into specific tools, it's crucial to understand the importance of efficient file formats and data partitioning.

### Parquet: The Champion of Big Data

Parquet has become the go-to file format for big data analytics, and for good reason. It's a columnar storage format that offers excellent compression and supports predicate pushdown, allowing for efficient querying of large datasets. When working with big data in R, converting your data to Parquet can lead to significant performance improvements.

### Partitioning: Divide and Conquer

Partitioning is a technique that divides large datasets into smaller, more manageable chunks. This approach enables parallel processing and efficient querying. A common partitioning strategy is to split data by date, creating a directory structure like `/year=2023/month=06/data.parquet`. This allows you to quickly filter data based on time periods without scanning the entire dataset.

Here's a quick example of how you can convert a large CSV file to partitioned Parquet using the Arrow package in R:

```         
library(arrow)
library(dplyr)

# Read CSV file
csv_dataset <- open_dataset("large_dataset.csv", format = "csv")

# Define partitioning schema and write to partitioned Parquet
write_dataset(csv_dataset, 
              "partitioned_dataset",
              format = "parquet",
              partitioning = schema(year = int32(), month = int32()))
```

This code reads a large CSV file and writes it as a partitioned Parquet dataset, organized by year and month.

## Arrow: The Swiss Army Knife of Big Data

Apache Arrow is a cross-language development platform for in-memory data, providing a standardized columnar memory format. The Arrow package in R brings this power to the R ecosystem.

### Key Features of Arrow:

1.  **Memory Mapping**: Allows efficient access to data without loading entire datasets into memory.

2.  **SIMD Operations**: Utilizes CPU's Single Instruction, Multiple Data capabilities for faster processing.

3.  **Streaming Execution**: Processes data in chunks, reducing memory footprint.

    Here's an example of using Arrow to efficiently query a partitioned Parquet dataset:

```         
library(arrow)

# Open partitioned dataset
dataset <- open_dataset("partitioned_dataset", format = "parquet")

# Efficient querying with partition and predicate pushdown
result <- dataset %>%
  filter(year == 2023, month == 6) %>%
  select(col1, col2) %>%
  collect()
```

This query leverages both partition pruning and predicate pushdown, ensuring that only the necessary data is read and processed.

## Polars: Speed and Efficiency in R

Polars is a lightning-fast DataFrames library implemented in Rust, with bindings available for R. It offers both eager and lazy execution modes, providing flexibility and performance.

### Polars Lazy Execution:

Polars' lazy execution mode allows for query optimization, potentially leading to significant performance improvements. Here's an example:

```         
library(polars)

# Create a lazy DataFrame with streaming enabled
lazy_df <- pl$lazy_csv_reader("large_dataset.csv", rechunk = FALSE)

# Define operations
result <- lazy_df$
  select(pl$col("col1"), pl$col("col2"))$
  filter(pl$col("col1") > 100)$
  group_by("col2")$
  agg(pl$col("col1")$mean())$
  collect(streaming = TRUE)
```

In this example, Polars optimizes the query plan and uses streaming execution to efficiently process the data.

## DuckDB: SQL Power for Local Data

DuckDB brings the power of a columnar-oriented SQL database to local, serverless environments. It's particularly well-suited for analytical queries on large datasets.

### DuckDB's Efficiency:

DuckDB optimizes for the entire memory hierarchy, from CPU cache to disk. It uses techniques like vectorized execution and adaptive algorithms to process data efficiently. Here's an example of using DuckDB to query a partitioned Parquet dataset:

```         
library(duckdb)
library(dplyr)

con <- dbConnect(duckdb())
result <- tbl(con, "parquet_scan('partitioned_dataset/**/*.parquet', 
                                 hive_partitioning=1)") %>%
  filter(year == 2023, month == 6) %>%
  select(col1, col2) %>%
  collect()

dbDisconnect(con, shutdown = TRUE)
```

DuckDB automatically detects the partitioning scheme and uses it for efficient data skipping.

## The Importance of Operation Order

When working with large datasets, the order of operations can significantly impact performance, especially in eager evaluation systems. Consider this example using dplyr:

```         
# Less efficient
df %>%
  group_by(category) %>%
  filter(value > 100) %>%
  summarise(mean_value = mean(value))

# More efficient
df %>%
  filter(value > 100) %>%
  group_by(category) %>%
  summarise(mean_value = mean(value))
```

In general, it's more efficient to filter data before grouping. However, lazy evaluation systems like Polars lazy and DuckDB can often optimize the query plan regardless of the order you specify operations.

## Summary: Choosing the Right Tool

Arrow, Polars, and DuckDB each offer unique strengths for handling large datasets in R:

-   **Arrow** excels in memory mapping and SIMD operations.

-   **Polars** shines with its lazy evaluation and query optimization.

-   **DuckDB** stands out for its adaptive algorithms and SQL engine

## Takeaways and Best Practices

When working with large datasets in R, consider these best practices:

1.  Use partitioned Parquet for storing large datasets.

2.  Leverage streaming capabilities to reduce memory footprint.

3.  Utilize automatic chunking and batch processing when available.

4.  Choose partitioning schemes that align with common query patterns.

5.  Benchmark different approaches for your specific use case.

6.  Pay attention to the order of operations, especially in eager evaluation systems

By employing these advanced techniques and tools, R users can efficiently handle datasets that were once considered too large for local processing. As the R ecosystem continues to evolve, we can expect even more powerful and efficient data handling capabilities in the future.

Happy data crunching!
